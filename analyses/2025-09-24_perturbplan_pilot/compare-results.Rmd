---
title: "compare-results"
output:
  pdf_document: default
  html_document: default
  output_dir: "../../results/2025-09-24_power_analysis"
editor_options: 
  chunk_output_type: console
---

# Compare perturbplan power analysis with simulation based power analysis

The data used for this comparison is the recent hiPSC -> HPC Day 0 dataset.
Count matrices were produced using cellranger v9.0.1 and gencode gtf v44.

The final SCEPTRE object was converted to  MuData format using the script `create-mudata-from-sceptre.Rmd`.

The simulation power analysis was run using `https://github.com/EngreitzLab/element-gene-power-analysis/tree/main`.

The `perturbplan` power analysis was run using the script `run_power_analysis.Rmd`. The `pvalue cutoff` used was the max p-value of the significant pairs in SCEPTRE.

[This is incorrect:](https://github.com/EngreitzLab/element-gene-power-analysis/blob/57f441106f7a6e6f388f7bd23c38f2e15eb87324/R/compute_power_from_simulations.R#L32)
Sceptre is not reporting FDR, using the code above you will get the raw pvalue.


```
cutoff <- max(sceptre_results$p_value[sceptre_results$significant], na.rm =TRUE)

cutoff = 0.1

Rscript run_power_analysis.R \
  -i ../../results/2025-09-24_power_analysis/mudata.h5mu \
  -o ../../results/2025-09-24_power_analysis/perturbplan_es_15_pval_0.1_left.tsv \
  --effect_size 0.85 \ #[0.50, 0.75, 0.80, 0.85]
  --effect_size_sd 0.13 \
  --pvalue_cutoff cutoff \
  --side left \
  -t 8
```


```{r,echo=FALSE, show=FALSE}
suppressPackageStartupMessages({
  library(data.table)
  library(dplyr)
  library(ggplot2)
  library(gridExtra)
  library(MuData)
  library(reshape2)
})

# Plot barplot function
plot_barplot_metrics <- function(summary_results, metric_prefix) {
  labels_df <- data.frame(
    metric = c(
      "n_pairs",
      "n_power_ge80",
      "frac_power_ge80",
      "expected_disc"
      ),
    label  = c(
      "Number of E-G pairs",
      "Number of pairs with power >= 0.80", 
      "Fraction of pairs with power >= 0.80",
      "Expected number of discoveries"
      )
  )
  
  metrics_to_plot <- names(summary_results)[grepl(metric_prefix, names(summary_results))]
  
  # Melt the summary results for ggplot
  summary_melted <- melt(summary_results, id.vars = NULL, measure.vars = metrics_to_plot)
  
  # For the labels remove the prefix 
  summary_melted$variable <- gsub(paste0("^", metric_prefix,"_"), "", summary_melted$variable)
  
  metric_long_label = labels_df$label[match(metric_prefix, labels_df$metric)]
  
  # Create the barplot and add values on top of bars
  ggplot(summary_melted, aes(x = variable, y = value)) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_text(aes(label = round(value, 2)), vjust = -0.5, position = position_dodge(width = 0.9)) +
    labs(
      title = metric_long_label,
      x = "Power computation method",
      y = "Value"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```


```{r warning=FALSE}
# Results from perturbplan
perturbplan_df = read.table("results/2025-09-24_power_analysis/perturbplan_es_15_pval_NULL_both.tsv", header=TRUE)

# Results from simulation
simulation_df = read.table("results/2025-09-24_power_analysis/power_analysis_results_es_0.15.tsv", header=TRUE)

# E-G scores from SCEPTRE
sceptre_df = readRDS("results/2025-09-19/sceptre_outputs/results_run_discovery_analysis.rds")

# Convert to data.table. Not really necessary but easier to work with.
perturb_dt <- as.data.table(perturbplan_df)
simulation_dt <- as.data.table(simulation_df)
sceptre_dt = as.data.table(sceptre_df)

# Clean up
rm(
  perturbplan_mudata,
  simulation_df,
  sceptre_df
)
```

```{r warning=FALSE}
# Merge all the variables to plot together
results_combined_df <- merge(
  perturb_dt[, .(grna_target, response_id, power_perturbplan = power)],
  simulation_dt[, .(grna_target, response_id, power_simulation = power)],
  by = c("grna_target", "response_id")
)

# Prepare SCEPTRE significance for merging
intermediate_for_merging <- sceptre_dt[, .(grna_target, response_id, sceptre_sig = significant)]


# Merge with t_merge
results_combined_df <- merge(
  results_combined_df,
  intermediate_for_merging,
  by = c("grna_target", "response_id")
)

head(results_combined_df)

# Clean up
rm(
  intermediate_for_merging,
  perturb_dt,
  simulation_dt,
  sceptre_dt
)
```

### Number of expected discoveries

```{r}
summary_results <- results_combined_df %>%
  summarise(
    n_pairs        = n(),
    n_power_ge80_perturbplan   = sum(power_perturbplan >= 0.80),
    frac_power_ge80_perturbplan= mean(power_perturbplan >= 0.80),
    expected_disc_perturbplan  = sum(power_perturbplan),
    n_power_ge80_simulation   = sum(power_simulation >= 0.80),
    frac_power_ge80_simulation= mean(power_simulation >= 0.80),
    expected_disc_simulation  = sum(power_simulation)
  )
print(summary_results)
```

```{r, fig.width=12}
# "n_pairs": "Total number of E-G pairs tested"
# "n_power_ge80": "Number of pairs with power >= 0.80"
# "frac_power_ge80":  "Fraction of pairs with power >= 0.80"
# "expected_disc": "Expected number of discoveries"

list_of_metrics = c("n_power_ge80", "frac_power_ge80", "expected_disc")

plots = lapply(list_of_metrics, plot_barplot_metrics, summary_results = summary_results)
do.call(grid.arrange, c(plots, ncol=3))
```


### Scatter plots of power results

It seems that the simulation-based power analysis agrees more with the SCEPTRE significance results.

There seems to be two population of pairs. One that has concordant power results between perturbplan and simulation, and another that has very low power in perturbplan but moderate to high power in simulation. 

Are there specific features of these pairs that explain this difference?
Possible explanations could be:
 - Difference in p-value cutoffs used to compute the power in the two methods
 - Different robustness of the methods caused by covariates (e.g. some guides have very few cells or UMI counts)
 
Results not shown - Requires further investigation) 
```{r, fig.height= 15}
p1 <- ggplot(results_combined_df, aes(x = power_perturbplan, y = power_simulation)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Power comparison: perturbplan vs simulation",
    subtitle = paste0("All E-G pairs. (n = ",nrow(results_combined_df),")"),
    x = "Power (perturbplan)",
    y = "Power (simulation)",
    ) +
  theme_minimal()

mask_sig = results_combined_df$sceptre_sig
p2 <- ggplot(results_combined_df[mask_sig,], aes(x = power_perturbplan, y = power_simulation)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "Power comparison: perturbplan vs simulation",
    subtitle = paste0("Only significant pairs in SCEPTRE. (n = ",nrow(results_combined_df[mask_sig,]),")"),
    x = "Power (perturbplan)",
    y = "Power (simulation)") +
  theme_minimal()

grid.arrange(p1, p2, ncol=1)
```





```{r}
library(dplyr)
# n_trt_per_grna: integer vector of treated-cell counts per gRNA for a target
# expr_mean: baseline gene mean (from baseline_expression_stats$expression_mean)
# fc_sd: fold_change_sd assumed in your power model
design_effect_summary <- function(n_trt_per_grna, expr_mean, fc_sd) {
  m      <- length(n_trt_per_grna)           # number of guides
  total  <- sum(n_trt_per_grna)
  s2     <- sum(n_trt_per_grna^2)
  ratio  <- s2 / (total^2)                    # appears in trt_var_across
  Neff   <- (total^2) / s2                    # effective number of guides
  deff   <- m * ratio                         # ≥1; variance inflation vs perfectly balanced
  tibble(
    m = m, 
    total_trt_cells = total,
    Neff = Neff, 
    eff_fraction = Neff / m,        # 1.0 means perfectly balanced
    ratio = ratio, 
    deff_vs_balanced = deff,
    # This multiplies the model's factor in the across-variance term:
    trt_var_across_coeff = (expr_mean^2) * (fc_sd^2) * ratio
  )
}


expr_mean <- 1.0
fc_sd     <- 0.20

design_effect_summary(c(100,100,100,100), expr_mean, fc_sd)   # balanced
design_effect_summary(c(50,100,150,200),  expr_mean, fc_sd)   # mild imbalance
design_effect_summary(c(10,10,10,420),    expr_mean, fc_sd)   # extreme imbalance


library(ggplot2)

simulate_designs <- function(total=400, m=4, k=2000) {
  set.seed(1)
  # draw many random allocations (Dirichlet-multinomial-like)
  A <- matrix(rexp(k*m, rate=1), ncol=m)     # i.i.d. exponentials -> Dirichlet after normalize
  alloc <- round(total * A / rowSums(A))
  rows <- lapply(seq_len(nrow(alloc)), function(i) {
    design_effect_summary(alloc[i,], expr_mean=1, fc_sd=0.2) %>%
      mutate(case=i)
  })
  bind_rows(rows)
}

df <- simulate_designs(total=400, m=6, k=1000)
ggplot(df, aes(eff_fraction)) +
  geom_histogram(bins=30) +
  labs(x="Neff / m (fraction of ‘usable’ guides)", y="Count",
       title="Imbalance reduces effective number of guides")








library(ggplot2)

simulate_designs <- function(total=400, m=4, k=2000) {
  set.seed(1)
  # draw many random allocations (Dirichlet-multinomial-like)
  A <- matrix(rexp(k*m, rate=1), ncol=m)     # i.i.d. exponentials -> Dirichlet after normalize
  alloc <- round(total * A / rowSums(A))
  rows <- lapply(seq_len(nrow(alloc)), function(i) {
    design_effect_summary(alloc[i,], expr_mean=1, fc_sd=0.2) %>%
      mutate(case=i)
  })
  bind_rows(rows)
}

df <- simulate_designs(total=400, m=6, k=1000)
ggplot(df, aes(eff_fraction)) +
  geom_histogram(bins=30) +
  labs(x="Neff / m (fraction of ‘usable’ guides)", y="Count",
       title="Imbalance reduces effective number of guides")



library(dplyr)
library(readr)
library(ggplot2)
# cells_per_grna is expected to have at least: grna_target, num_cells
# (and includes a row group with grna_target == "non-targeting" which we exclude)

design_effect_by_target <- cells_per_grna %>%
  filter(grna_target != "non-targeting") %>%
  group_by(grna_target) %>%
  summarise(
    m                 = n(),                                   # number of guides for this target
    total_trt_cells   = sum(num_cells, na.rm = TRUE),
    sumsq_trt_cells   = sum((num_cells %>% replace_na(0))^2),  # ∑ n_i^2
    .groups = "drop"
  ) %>%
  mutate(
    ratio             = sumsq_trt_cells / (total_trt_cells^2),       # imbalance term
    Neff              = (total_trt_cells^2) / sumsq_trt_cells,       # effective # of guides
    eff_fraction      = Neff / m,                                    # 1.0 when perfectly balanced
    deff_vs_balanced  = m * ratio                                    # ≥1; variance inflation vs balance
  ) %>%
  arrange(eff_fraction)

# Peek: worst 10 (most imbalanced) and best 10 (most balanced)
worst10 <- head(design_effect_by_target, 10)
best10  <- tail(design_effect_by_target, 10)

print(worst10)
print(best10)

# Save a CSV you can inspect / share
write_csv(design_effect_by_target, "design_effect_by_target.csv")






ggplot(design_effect_by_target, aes(eff_fraction)) +
  geom_histogram(bins = 30, color = "white") +
  labs(x = "Neff / #guides (fraction of usable guides)",
       y = "Number of targets",
       title = "Imbalance across guides per target") +
  theme_bw()

fc_sd <- 0.20  # example; use whatever you pass to your power function

design_effect_by_target <- design_effect_by_target %>%
  mutate(trt_var_across_coeff = ratio * (fc_sd^2))

by_target_power <- res$individual_power %>%
  group_by(grna_target) %>%
  summarise(expected_disc_target = sum(power),
            avg_power = mean(power),
            frac_ge80 = mean(power >= 0.80),
            .groups = "drop")

plot_df <- design_effect_by_target %>%
  inner_join(by_target_power, by = "grna_target")

ggplot(plot_df, aes(eff_fraction, avg_power)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "Neff / #guides (balance ↑ → 1.0)", y = "Mean power per target",
       title = "Guide balance vs. power") +
  theme_bw()
```